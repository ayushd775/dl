{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c09cef-f414-438b-9094-eceeab5eb0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 01:40:41.998459: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-10 01:40:42.043370: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-10 01:40:43.198246: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 01:40:43.799776: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4786 - mae: 0.4833 - val_loss: 0.2396 - val_mae: 0.3309\n",
      "Epoch 2/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1851 - mae: 0.2940 - val_loss: 0.1543 - val_mae: 0.2645\n",
      "Epoch 3/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1313 - mae: 0.2478 - val_loss: 0.1216 - val_mae: 0.2327\n",
      "Epoch 4/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1030 - mae: 0.2181 - val_loss: 0.1032 - val_mae: 0.2112\n",
      "Epoch 5/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0892 - mae: 0.2022 - val_loss: 0.0952 - val_mae: 0.2011\n",
      "Epoch 6/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0826 - mae: 0.1947 - val_loss: 0.0917 - val_mae: 0.1976\n",
      "Epoch 7/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0786 - mae: 0.1903 - val_loss: 0.0870 - val_mae: 0.1913\n",
      "Epoch 8/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0750 - mae: 0.1858 - val_loss: 0.0840 - val_mae: 0.1893\n",
      "Epoch 9/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0722 - mae: 0.1824 - val_loss: 0.0829 - val_mae: 0.1873\n",
      "Epoch 10/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0698 - mae: 0.1794 - val_loss: 0.0793 - val_mae: 0.1828\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "Threshold:  0.1634843274096918\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2069   10]\n",
      " [ 146 2773]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Anomaly       0.93      1.00      0.96      2079\n",
      "      Normal       1.00      0.95      0.97      2919\n",
      "\n",
      "    accuracy                           0.97      4998\n",
      "   macro avg       0.97      0.97      0.97      4998\n",
      "weighted avg       0.97      0.97      0.97      4998\n",
      "\n",
      "Accuracy:  0.9687875150060024\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# a. Load and preprocess ECG dataset\n",
    "data = pd.read_csv('/home/ayush/Desktop/lp4/dataset/ecg_autoencoder_dataset.csv', header=None)\n",
    "\n",
    "# Last column (140) is the class label, rest are features\n",
    "X = data.iloc[:, :-1].values  # All columns except last\n",
    "y = data.iloc[:, -1].values   # Last column is the label (0=anomaly, 1=normal)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Separate normal data (Class = 1) for training the autoencoder\n",
    "x_train = X_scaled[y == 1]  # Train only on normal heartbeats\n",
    "\n",
    "# Test data (normal + anomalies)\n",
    "x_test = X_scaled\n",
    "y_test = y\n",
    "\n",
    "# b. Build Autoencoder model\n",
    "input_dim = x_train.shape[1]  # 140 features\n",
    "inp = Input((input_dim,))\n",
    "enc = Dense(64, activation='relu')(inp)\n",
    "lat = Dense(32, activation='relu')(enc)\n",
    "dec = Dense(64, activation='relu')(lat)\n",
    "out = Dense(input_dim, activation='linear')(dec)\n",
    "model = Model(inp, out)\n",
    "\n",
    "# c. Compile & Train\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "history = model.fit(x_train, x_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# d. Reconstruction on test data\n",
    "reconstructions = model.predict(x_test)\n",
    "\n",
    "# e. Calculate reconstruction error\n",
    "mse = np.mean(np.power(x_test - reconstructions, 2), axis=1)\n",
    "\n",
    "# f. Set threshold for anomaly detection (using 95th percentile of training errors)\n",
    "train_reconstructions = model.predict(x_train)\n",
    "train_mse = np.mean(np.power(x_train - train_reconstructions, 2), axis=1)\n",
    "threshold = np.percentile(train_mse, 95)\n",
    "print(\"Threshold: \", threshold)\n",
    "\n",
    "# g. Predict anomalies (mse > threshold means anomaly, so predict 0)\n",
    "y_pred = (mse > threshold).astype(int)  # 1 if mse > threshold (anomaly)\n",
    "y_pred = 1 - y_pred  # Flip: 0=anomaly, 1=normal to match original labels\n",
    "\n",
    "# h. Evaluation\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Anomaly', 'Normal']))\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc3da6eb-4b5c-4fc3-a137-b24341e0d92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m7108/7108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 908us/step - loss: 0.4921 - mae: 0.4484 - val_loss: 0.3762 - val_mae: 0.4006\n",
      "Epoch 2/5\n",
      "\u001b[1m7108/7108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 862us/step - loss: 0.3598 - mae: 0.3847 - val_loss: 0.3531 - val_mae: 0.3865\n",
      "Epoch 3/5\n",
      "\u001b[1m7108/7108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 871us/step - loss: 0.3361 - mae: 0.3683 - val_loss: 0.3308 - val_mae: 0.3694\n",
      "Epoch 4/5\n",
      "\u001b[1m7108/7108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 874us/step - loss: 0.3286 - mae: 0.3619 - val_loss: 0.3280 - val_mae: 0.3728\n",
      "Epoch 5/5\n",
      "\u001b[1m7108/7108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 873us/step - loss: 0.3241 - mae: 0.3587 - val_loss: 0.3247 - val_mae: 0.3681\n",
      "\u001b[1m8901/8901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 400us/step\n",
      "Threshold:  0.7450179863458147\n",
      "\n",
      "Confusion Matrix:\n",
      "[[270510  13805]\n",
      " [    56    436]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       1.00      0.95      0.98    284315\n",
      "       Fraud       0.03      0.89      0.06       492\n",
      "\n",
      "    accuracy                           0.95    284807\n",
      "   macro avg       0.52      0.92      0.52    284807\n",
      "weighted avg       1.00      0.95      0.97    284807\n",
      "\n",
      "Accuracy:  0.9513319546219019\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# a. Load and preprocess dataset\n",
    "data = pd.read_csv('/home/ayush/Desktop/lp4/dataset/creditcard.csv')\n",
    "data['Amount'] = StandardScaler().fit_transform(data[['Amount']])\n",
    "data = data.drop(['Time'], axis=1)\n",
    "\n",
    "# Separate normal transactions (Class = 0) for training\n",
    "x_train = data[data['Class'] == 0].drop(['Class'], axis=1).values\n",
    "\n",
    "# Test data (normal + fraud)\n",
    "x_test = data.drop(['Class'], axis=1).values\n",
    "y_test = data['Class'].values\n",
    "\n",
    "# b. Build Autoencoder model\n",
    "inp = Input((29,))\n",
    "enc = Dense(16, activation='relu')(inp)\n",
    "lat = Dense(8, activation='relu')(enc)\n",
    "dec = Dense(16, activation='relu')(lat)\n",
    "out = Dense(29, activation='linear')(dec)\n",
    "model = Model(inp, out)\n",
    "\n",
    "# c. Compile & Train\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.fit(x_train, x_train, epochs=5, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# d. Reconstruction on test data\n",
    "reconstructions = model.predict(x_test)\n",
    "\n",
    "# e. Calculate reconstruction error\n",
    "mse = np.mean(np.power(x_test - reconstructions, 2), axis=1)\n",
    "\n",
    "# f. Set threshold for anomaly detection\n",
    "threshold = np.percentile(mse, 95)  # top 5% errors = fraud\n",
    "print(\"Threshold: \", threshold)\n",
    "\n",
    "# g. Predict anomalies\n",
    "y_pred = (mse > threshold).astype(int)\n",
    "\n",
    "# h. Evaluation\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Normal', 'Fraud']))\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3f5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
